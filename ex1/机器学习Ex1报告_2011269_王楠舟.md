# Exercise1-Softmax Regression

**2011269 王楠舟 计算机科学与技术** 

## 实验目的

在本次实验中，需要训练一个分类器来完成对MNIST数据集中`0-9`10个手写数字的分类。

## 程序基本流程

```python
 # get the data
    train_images, train_labels, test_images, test_labels = load_data(mnist_dir, train_data_dir, train_label_dir, test_data_dir, test_label_dir)
    print("Got data. ") 
```

首先从指定好的路径中将手写数字数据集读入训练数据集和测试数据集。

```python
    # train the classifier
    theta = train(train_images, train_labels, k, iters, alpha)
    print("Finished training. ") 
```

随后调用`train`函数，训练出目标模型权重`theta`。

```python
    # evaluate on the testset
    y_predict = predict(test_images, theta)
    accuracy  = cal_accuracy(y_predict, test_labels)
    print(accuracy)
    print("Finished test. ") 
```

最后利用训练出来的模型权重计算训练集的准确率。

我们需要补全的代码部分包括`softmax_regression.py`和`evaluate.py`两个文件。

## 程序实现原理

首先，每一张手写数字都是`28x28`维的，对其展开后变成`784x1`的列向量`x`，模型权重是一个`10x784`维的矩阵`theta`，`theta·x=y_output`，得到的结果`y_output`是一个`10x1`的列向量。

接下来对`y_output`进行`softmax`函数处理，`y`中的每一个元素的softmax值如下公式所示：

<img src="C:\Users\LEGION\Desktop\机器学习\image-20221103221252270.png" alt="image-20221103221252270" style="zoom:67%;" />

`softmax`函数返回结果列向量`y^`中，每一个元素的值代表模型预测`x`是数字几的概率为多少，随后将得到的`y^`与真实标签`labels`计算损失值，采用的损失函数是交叉熵。

<img src="C:\Users\LEGION\Desktop\机器学习\image-20221103222511295.png" alt="image-20221103222511295" style="zoom:80%;" />

得到的`J(θ)`就是本次训练的损失值，接下来对损失函数求导计算模型梯度`g`，如下：

<img src="C:\Users\LEGION\Desktop\机器学习\image-20221103222426063.png" alt="image-20221103222426063" style="zoom:80%;" />

最后用模型梯度`g`更新模型权重`theta`，其中`alpha`是学习率：`theta = theta - alpha * g`

## 具体代码实现

### `evaluate.py`代码实现

```python
def cal_accuracy(y_pred, y):
    # TODO: Compute the accuracy among the test set and store it in acc
    num = 0.0
    for i in range (len(y)):
        if(y_pred[i] == y[i]):
            num += 1
    acc = num / len(y)
    return acc
```

计算在测试数据上的准确率就是比较预测结果`y_pred`和真实label`y`之间有多少是相同的，最后除以测试数据集大小，就是在测试数据集上的模型准确率。

### `Softmax_Regression`代码实现

```python
def softmax(output):
    output_exp=np.exp(output)
    exp_sum=output_exp.sum(axis=0)
    return (output_exp/exp_sum)#转化为softmax值形式
```

首先定义一个函数用于将模型输出`theta·x`转化为`softmax`值的形式。

```python
def loss(y,y_hat):
    batch_size=y.shape[1]
    y_hat=np.log(y_hat)
    l_sum=0.0
    for i in range(batch_size):
        l_sum+=np.dot(y_hat[:,i].T,y[:,i])
        #转化为行向量乘列向量形式
    return -(1.0/batch_size)*l_sum
```

定义函数`loss`计算`softmax`转化后的`y_hat`与真实label`y`的损失值。

```python
def gradient(y,y_hat,data):
    batch_size=y.shape[1]
    return -(1.0/batch_size)*np.dot((y-y_hat),data.T)
```

定义函数`gradient`计算模型梯度。

```
def softmax_regression(theta, x, y, iters, alpha):

    #the list of LOSS
    f = list()
    #个人习惯，可以不转置
    data=x.T

    for i in range(iters):
        out=output(theta,data)
        #print(out.shape)
        y_hat=softmax(out)
        train_loss=loss(y,y_hat)
        
        f.append(train_loss)
        
        #g is the gradient
        g=gradient(yj,y_hat,data)
        theta = theta - alpha * g
        
        return theta
```

最后`softmax_regression`函数实现如上代码所示，在每一轮迭代中使用全部的训练数据集`data`对模型`theta`进行训练，最后得到的准确率有`0.90`左右。

### 代码改进

1. **分批次训练**：在每一轮迭代训练时，将训练集数据划分为不同批次数据进行训练
2. **增加正则项**

最后改进完成的代码如下，其中损失函数加上正则项`lam*np.sum(theta**2)`，在计算梯度时同样需要加上正则项求导结果` lam*theta`.

```python
def softmax_regression(theta, x, y, iters, alpha):
    # TODO: Do the softmax regression by computing the gradient and 
    # the objective function value of every iteration and update the theta

    #the list of LOSS
    f = list()

    #惩罚项的力度
    lam = 0.0000025
    #个人习惯，可以不转置
    data=x.T
    #分批次训练
    batch_size = 200
    epoch=int(len(x)/batch_size)
    print(iters*epoch)
    
    for i in range(iters):
        for j in range(epoch):
            dataj=data[:,j*batch_size:(j+1)*batch_size]
            out=output(theta,dataj)
            y_hat=softmax(out)
            yj=y[:,j*batch_size:(j+1)*batch_size]
            
            train_loss=loss(yj,y_hat) + lam*np.sum(theta**2)
            #print(train_loss,lam*np.sum(theta**2))
            if j==epoch-1:
                f.append(train_loss)

            #g is the gradient
            g=gradient(yj,y_hat,dataj) + lam*theta
            theta = theta - alpha * g

    return theta
```

## 实验结果及分析

超参数的选择：`iters = 150, alpha = 0.15, lam=0.0000025, batch_size = 200`

实验结果：

<img src="C:\Users\LEGION\Desktop\机器学习\image-20221103231314234.png" alt="image-20221103231314234" style="zoom:80%;" />

准确率达到`0.92`说明模型训练比较好。

最后我平均了五次训练的结果，最差准确率为`0.9195`，最好达到`0.9214`，结果最后稳定在`0.92`附近，说明模型设计比较合理。

<img src="C:\Users\LEGION\Desktop\机器学习\test.png" alt="test" style="zoom:12%;" />

最后根据`f`画出损失值随迭代次数的变化结果，可见损失值收敛速度比较快，且在迭代到140次左右损失值就收敛了，说明超参数选择比较好。

**拓展思考**

我还尝试加入了一个偏置常量`bias`在模型训练中，但最后模型效果反而变差了，准确率只有`0.90`左右，遂放弃；

我还尝试了在不同迭代中将训练集打乱，在未改进前的版本能将准确率提高到`0.91`左右，但在分批次训练中几乎没有提升准确率，遂没有给出代码。

GitHub仓库链接：[](https://github.com/Stupid-wangnz/ML)

